[global]
seed = 13
device = cuda
; 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

[train]
epoch = 100
batch_size = 32
lr = 1e-3
shuffle = True
num_worker = 0
criterion = MSELoss
feature_loss_weight = 1, 0.001, 0.0001
optimizer = AdamW
; SGD
; AdamW
lr_scheduler = ReduceLROnPlateau
; CosineAnnealingLR
; ReduceLROnPlateau
; OneCycleLR
teacher_forcing_ratio = 0.5

[eval]
batch_size = 8
shuffle = False
num_worker = 0

[preprocess]
input_norm_type = min_max, min_max, min_max, min_max
; min_max
; z_score
; None
truth_norm_type = None, min_max, min_max, min_max

[data]

train_year = 2018, 2019
test_year = 2020, 2021
valid_ratio = 0.8

dataset_type = SWGIMDataset

; using features
input_features = tec
; year, DOY, hour, kp, r, dst, ap, f10.7, storm_state, storm_size, tec, tec_sh;
truth_features = tec, dst
; year, DOY, hour, kp, r, dst, ap, f10.7, storm_state, storm_size, tec, tec_sh;
seq_base = longitude
; time or latitude, longitude
; tec_sh seq_base only type time

reduce = False
reduce_ratio = 0.2

[model]
model_name = Transformer_E_dst
; LSTM_TEC
; LSTM_Seq2Seq_TEC
; Transformer_E
; Transformer_ED
input_time_step = 24
; model input x hours TEC
output_time_step = 1
; model output x-hour-later TEC
embedding_size = 512
; ignore in Transformer (equal to hidden_size)
; for seq2seq model
hidden_size = 128
num_layer = 12
dropout = 0.5

[output]
output_func = SWGIM
rounding_digit = 5