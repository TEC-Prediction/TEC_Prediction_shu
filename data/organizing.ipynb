{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "def read_csv_data(years, DATAPATH=Path('./raw_data/SWGIM_year/')):\n",
    "    reserved_point = [(67.5, -65), (25, 120), (0, -90), (-20, -160), (-32.5, 20), (-77.5, 165)]\n",
    "    \n",
    "    droplist = [0] + list(range(9, 10235))\n",
    "    for lat, lng in reserved_point:\n",
    "        droplist.remove(10 + int((87.5-lat)/2.5)*72 + int((180+lng)/5+1))\n",
    "        \n",
    "    renamelist = ['year', 'DOY', 'hour', 'Kp index',\n",
    "            'R', 'Dst-index, nT', 'ap_index, nT', 'f10.7_index', (67.5, -65),\\\n",
    "                            (25, 120), (0, -90), (-20, -160), (-32.5, 20), (-77.5, 165)]\n",
    "\n",
    "    df_list = []\n",
    "    print('Reading csv data...')\n",
    "    for year in tqdm(years):\n",
    "        year_df = pd.read_csv(DATAPATH / Path(f'{year}.csv'), header=list(range(6)))\n",
    "\n",
    "        # drop columns\n",
    "        year_df.drop(year_df.columns[droplist], inplace=True, axis=1, errors='ignore')\n",
    "        \n",
    "        # rename dataframe\n",
    "        year_df.columns = renamelist\n",
    "        \n",
    "        df_list.append(year_df)\n",
    "        \n",
    "    all_df = pd.concat(df_list, axis=0)\n",
    "        \n",
    "    return all_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_df = pd.read_csv('./single_point_test.csv')\n",
    "train_df.to_csv('./single_point_test.csv')\n",
    "train_df = pd.read_csv('./single_point_train.csv')\n",
    "train_df.to_csv('./single_point_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2015: 18254 - 18774\n",
    "# 2016: 18775 - 19296"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2192/2192 [1:05:14<00:00,  1.79s/it]\n",
      "100%|██████████| 6/6 [00:04<00:00,  1.28it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import threading\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=pd.errors.PerformanceWarning)\n",
    "\n",
    "def ion2list(ion_filename):\n",
    "    \n",
    "    daily_text = open(ion_filename, 'r').read()\n",
    "    \n",
    "    year, doy = daily_text[43:43+4], daily_text[38:38+3]\n",
    "        \n",
    "    # print(year, doy, hour)\n",
    "    hour_len = 12357\n",
    "    \n",
    "    daily_list = []\n",
    "    # each hour\n",
    "    for hour in range(24):\n",
    "        hourly_text = daily_text[hour_len*hour:hour_len*(hour+1)]\n",
    "        \n",
    "        line_len, header_len = 42, 1604\n",
    "        value_list, rms_list = [], []\n",
    "        for l in range(256):\n",
    "            line_text = hourly_text[header_len + l * line_len:header_len + (l+1) * line_len]\n",
    "            \n",
    "            value_list.append(float(line_text[19:19+11]))\n",
    "            rms_list.append(float(line_text[35:35+6]))\n",
    "            \n",
    "        daily_list.append(value_list + rms_list)\n",
    "    \n",
    "    return int(doy), np.array(daily_list) # 24,512\n",
    "   \n",
    "def download_ion_file(year, ion_filename):\n",
    "    \n",
    "    baseurl =  'http://ftp.aiub.unibe.ch/CODE'\n",
    "    \n",
    "    # download ion file\n",
    "    open(ion_filename, 'wb').write(requests.get(f\"{baseurl}/{year}/{ion_filename}\").content)\n",
    "    \n",
    "    # unzip .Z file\n",
    "    os.system(f'uncompress {ion_filename}')\n",
    "              \n",
    "    # os.system(f'rm {ion_filename}')\n",
    "\n",
    "# download_ion_file(2015, 'COD18260.ION.Z')\n",
    "\n",
    "def daily_data_2_df(year, ion_filename, SWGIM_filename, label_storm_df, header_df):\n",
    "\n",
    "    # read SWGIM df\n",
    "    SWGIM_day = pd.read_csv(SWGIM_filename, index_col=0, header=list(range(6))).reset_index(drop=True)\n",
    "    # print(SWGIM_day.info())\n",
    "    # print(SWGIM_day.head())\n",
    "    # download ion file\n",
    "    download_ion_file(year, ion_filename)\n",
    "    \n",
    "    # read ion file\n",
    "    doy, daily_ion_list = ion2list(ion_filename[:-2])\n",
    "    \n",
    "    os.system(f'rm {ion_filename[:-2]}')\n",
    "    \n",
    "    label_daily_df = label_storm_df.loc[(label_storm_df['year'] == year) & (label_storm_df['DOY'] == doy)].iloc[:,3:5].reset_index(drop=True)\n",
    "    # print(label_daily_df)\n",
    "\n",
    "    # fill label\n",
    "    SWGIM_day[('label', 'Geomagnetic Storms Size')] = label_daily_df['Geomagnetic Storms Size']\n",
    "    SWGIM_day[('label', 'Geomagnetic Storms State')] = label_daily_df['Geomagnetic Storms State']\n",
    "    \n",
    "    # fill sh coef & RMSE\n",
    "    orderlist = [0] \n",
    "    for k in zip(range(1, 16), range(-1,-16,-1)):\n",
    "        orderlist += [k[0] , k[1]]\n",
    "    \n",
    "    # SH coef.\n",
    "    idx = 0\n",
    "    for degree in range(16):\n",
    "        for order in orderlist[:2*degree+1]:\n",
    "            col_name = ('CODE','SH coef','-','f2.8',str(degree),str(order))\n",
    "            # assert col_name in SWGIM_day.columns, f'{col_name} not in {year} {doy} columns!'\n",
    "            SWGIM_day[col_name] = daily_ion_list[:,idx]\n",
    "            idx += 1\n",
    "    # SH coef. RMSE\n",
    "    idx = 0\n",
    "    for degree in range(16):\n",
    "        for order in orderlist[:2*degree+1]:\n",
    "            col_name = ('CODE','SH coef RMSE','-','f1.4',str(degree),str(order))\n",
    "            # assert col_name in SWGIM_day.columns, f'{col_name} not in {year} {doy} columns!'\n",
    "            SWGIM_day[col_name] = daily_ion_list[:,idx]\n",
    "            idx += 1\n",
    "    \n",
    "    SWGIM_day.columns = header_df.columns\n",
    "    # SWGIM_day.to_csv('./temp1.csv')\n",
    "    SWGIM_day.to_csv(SWGIM_filename)\n",
    "    \n",
    "# label_storm_df = pd.read_csv('./raw_data/23label_all_-10.csv', index_col=0, usecols=list(range(6)))\n",
    "# headers df\n",
    "# header_df = pd.read_csv('./raw_data/SWGIM_headers.csv', index_col=0, header=list(range(6)))    \n",
    "# daily_data_2_df(2015, 'COD18256.ION.Z', './raw_data/SWGIM_day/2015/2015003.csv', label_storm_df, header_df)\n",
    "# print(label_storm_df.head())\n",
    "\n",
    "import re\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_filenames(year):\n",
    "    baseurl = 'http://ftp.aiub.unibe.ch/CODE'\n",
    "    sauce = requests.get(f'{baseurl}/{year}')\n",
    "    \n",
    "    soup = BeautifulSoup(sauce.text,'html.parser')\n",
    "    \n",
    "    filenames = [re.match('COD\\d{5}\\.ION\\.Z', a.text) for a in soup.find_all('a')]\n",
    "    filenames = [a.group() for a in filenames if a is not None]\n",
    "    \n",
    "    # print(len(filename))\n",
    "    return filenames\n",
    "# get_filenames(2015)\n",
    "\n",
    "def main():\n",
    "    \n",
    "    # read storm df\n",
    "    label_storm_df = pd.read_csv('./raw_data/23label_all_-10.csv', index_col=0, usecols=list(range(6))).reset_index(drop=True)\n",
    "    # display(label_storm_df.info())\n",
    "    \n",
    "    # headers df\n",
    "    header_df = pd.read_csv('./raw_data/SWGIM_headers.csv', index_col=0, header=list(range(6)))\n",
    "    \n",
    "    SWGIM_path = './raw_data/SWGIM_day'\n",
    "    \n",
    "    args_list = [(year, a, f'{SWGIM_path}/{year}/{year}{idx+1:03d}.csv')\\\n",
    "        for year in range(2016,2022) for idx, a in enumerate(get_filenames(year))]\n",
    "    # print(args_list)\n",
    "    \n",
    "    # for args in args_list[:5]:\n",
    "    #     daily_data_2_df(*args, label_storm_df, header_df)\n",
    "    # return    \n",
    "    threads = []\n",
    "    thread_limit = 10\n",
    "\n",
    "    for args in tqdm(args_list):\n",
    "        while threading.active_count() > thread_limit:\n",
    "            threads[0].join()\n",
    "            threads.pop(0)\n",
    "\n",
    "        threads.append(threading.Thread(target = daily_data_2_df,\\\n",
    "            args = (*args, label_storm_df, header_df)))\n",
    "        threads[-1].start()\n",
    "\n",
    "    for thread in tqdm(threads):\n",
    "        thread.join()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 365/365 [06:15<00:00,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 365/365 [06:14<00:00,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 366/366 [06:16<00:00,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 365/365 [06:17<00:00,  1.03s/it]\n"
     ]
    }
   ],
   "source": [
    "basepath = Path('./raw_data/SWGIM_day')\n",
    "\n",
    "# headers df\n",
    "header_df = pd.read_csv('./raw_data/SWGIM_headers.csv', index_col=0, header=list(range(6)))\n",
    "    \n",
    "for year in range(2018,2022):\n",
    "    print(year)\n",
    "    year_list = []\n",
    "    for p in tqdm(sorted((basepath / Path(f'{year}')).glob('*'))):\n",
    "        df = pd.read_csv(p, index_col=0, header=list(range(6)))\n",
    "        \n",
    "        # df.columns = header_df.columns[:len(df.columns)]\n",
    "        df.to_csv(p)\n",
    "        year_list.append(df)\n",
    "    \n",
    "    # year_df = pd.read_csv(f'./raw_data/SWGIM_year/{year}.csv', index_col=0, header=list(range(6)))\n",
    "    # year_df.columns = header_df.columns[:len(year_df.columns)]\n",
    "    year_df = pd.concat(year_list, ignore_index=True)\n",
    "    year_df.to_csv(f'./raw_data/SWGIM_year/{year}.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8784 entries, 0 to 8783\n",
      "Columns: 10234 entries, ('UTC', 'year', '-', 'I4', 'global', 'global') to ('CODE', 'GIM RMS', '10TEC', 'I3', '-87.5', '175')\n",
      "dtypes: float64(10227), int64(7)\n",
      "memory usage: 685.8 MB\n",
      "None\n",
      "Source                UTC                OMNIWeb                            \\\n",
      "Feature              year    DOY   hour Kp index           R Dst-index, nT   \n",
      "unit                    -      -      -        - Sunspot No.            nT   \n",
      "format                 I4     I3     I2       I2          I3            S3   \n",
      "location latitude  global global global   global      global        global   \n",
      "location longitude global global global   global      global        global   \n",
      "0                    2000    238      0        0         112            -9   \n",
      "1                    2000    238      1        0         112            -7   \n",
      "2                    2000    238      2        0         112            -7   \n",
      "3                    2000    238      3       10         112            -5   \n",
      "4                    2000    238      4       10         112            -2   \n",
      "\n",
      "Source                                                        label  \\\n",
      "Feature            ap_index, nT f10.7_index Geomagnetic Storms Size   \n",
      "unit                         nT       index                   Level   \n",
      "format                       I2        F3.1                      I1   \n",
      "location latitude        global      global                  global   \n",
      "location longitude       global      global                  global   \n",
      "0                             0       136.0                     NaN   \n",
      "1                             0       136.0                     NaN   \n",
      "2                             0       136.0                     NaN   \n",
      "3                             4       136.0                     NaN   \n",
      "4                             4       136.0                     NaN   \n",
      "\n",
      "Source                                       ...    CODE                    \\\n",
      "Feature            Geomagnetic Storms State  ... GIM RMS                     \n",
      "unit                               AR state  ...   10TEC                     \n",
      "format                                   S1  ...      I3                     \n",
      "location latitude                    global  ...   -87.5                     \n",
      "location longitude                   global  ...     130   135   140   145   \n",
      "0                                       NaN  ...     NaN   NaN   NaN   NaN   \n",
      "1                                       NaN  ...    10.0  10.0  10.0  10.0   \n",
      "2                                       NaN  ...     NaN   NaN   NaN   NaN   \n",
      "3                                       NaN  ...     9.0   9.0   9.0   9.0   \n",
      "4                                       NaN  ...     NaN   NaN   NaN   NaN   \n",
      "\n",
      "Source                                                  \n",
      "Feature                                                 \n",
      "unit                                                    \n",
      "format                                                  \n",
      "location latitude                                       \n",
      "location longitude   150   155   160   165   170   175  \n",
      "0                    NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "1                   10.0  10.0  10.0  10.0  10.0  10.0  \n",
      "2                    NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "3                    9.0   9.0   9.0   9.0   9.0   9.0  \n",
      "4                    NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "\n",
      "[5 rows x 10234 columns]\n"
     ]
    }
   ],
   "source": [
    "print(year_df.info())\n",
    "print(year_df.head())\n",
    "# print(year_df.iloc[24:72:2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
